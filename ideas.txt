
We have implemented a script that takes the data coded with the format specified in the document by noaa.
Here is an example of decoded record.
We have parsed all the mandatory fields, to understand if there were any order on the data.
We understood that datas are divided into groups by years. Each year contains a file for each station that recorded some data in that year. Inside each file the records are ordered by date and time.
We have also normalized the values to their correct unit-measure by dividing them with scaling factor provided in the document. We have identified and changed the values representing empty values to empty values.
Then we selected all the data that can be useful for getting information about the global warming. The interesting data are: location, date, air temperature, sea level and liquid precipitation.


We want to calculate average, median, max, min of the data for each station
    or grouping the stations that are close. To do that we compute the results for various data range.
    The results data should be small enough to be possible to mantain different result sets.
    to compute them we can start by calculating the statistic result on the smallest data range (ex. 1 day or 1 month) and from this compute it for larger data ranges.

The tools we have planned to use for the moment are only Spark and HDFS. The computations are simple, they don't need any particular machine learning algorithm or graph processing tool.

For the user interface we plan to use google maps, showing on top of it a heatmap that shows the different statistics of temperature and some indicators to shows irregular changes in the sea level and precipitations.
We plan to give also the feature of pressing in one point of the map to see the textual statistics for that particular zone. The map will let the user also to change the statistic that is currently displayed and the data range of the statistic.
The map will be displayed in a web browser. So we will use a web framework to build the web application.